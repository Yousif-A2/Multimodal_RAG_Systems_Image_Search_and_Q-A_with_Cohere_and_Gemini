{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yousif-A2/Multimodal_RAG_Systems_Image_Search_and_Q-A_with_Cohere_and_Gemini/blob/main/Visual_Memory_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-xq4flOtV76"
      },
      "source": [
        "# ðŸ§  Visual Memory Assistant\n",
        "\n",
        "Welcome to the **Visual Memory Assistant**, an AI-powered tool that helps you search your personal photo collection using natural language. Upload your photos, and ask questions like:\n",
        "\n",
        "- *\"Show me the photo from that cafe in Istanbul.\"*\n",
        "- *\"Which one has my red jacket?\"*\n",
        "- *\"Find the picture from our last winter trip.\"*\n",
        "\n",
        "### ðŸ’¡ Powered By\n",
        "- **Cohere Embed v4.0** for visual memory embeddings\n",
        "- **Gemini** for natural language Q&A over images\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBZh9P63TfUN",
        "outputId": "bad76ac5-6ed1-49cf-f224-8a786327df93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.16.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.11.7)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.1.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Downloading cohere-5.16.1-py3-none-any.whl (291 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m291.9/291.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.16.1 fastavro-1.11.1 httpx-sse-0.4.0 types-requests-2.32.4.20250611\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8Mi8u2fTSw0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT9qcg0yS8oi"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "cohere_api_key = userdata.get('coher')\n",
        "co = cohere.ClientV2(api_key=cohere_api_key)\n",
        "\n",
        "import google.generativeai as genai\n",
        "gemini_api_key = userdata.get('gemini')\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "client = genai.GenerativeModel('gemini-2.0-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsXslBuvTApo"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import io\n",
        "import base64\n",
        "\n",
        "max_pixels = 1568*1568  # Max resolution for images\n",
        "\n",
        "# Resize too large images\n",
        "def resize_image(pil_image):\n",
        "    org_width, org_height = pil_image.size\n",
        "    if org_width * org_height > max_pixels:\n",
        "        scale_factor = (max_pixels / (org_width * org_height)) ** 0.5\n",
        "        new_width = int(org_width * scale_factor)\n",
        "        new_height = int(org_height * scale_factor)\n",
        "        pil_image.thumbnail((new_width, new_height))\n",
        "\n",
        "# Convert images to a base64 string\n",
        "def base64_from_image(img_path):\n",
        "    pil_image = PIL.Image.open(img_path)\n",
        "    img_format = pil_image.format if pil_image.format else \"PNG\"\n",
        "\n",
        "    resize_image(pil_image)\n",
        "\n",
        "    with io.BytesIO() as img_buffer:\n",
        "        pil_image.save(img_buffer, format=img_format)\n",
        "        img_buffer.seek(0)\n",
        "        img_data = f\"data:image/{img_format.lower()};base64,\"+base64.b64encode(img_buffer.read()).decode(\"utf-8\")\n",
        "\n",
        "    return img_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyXvrVQuTEuT",
        "outputId": "20360a67-dd44-4c8d-c705-208e7b117f77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.95s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "# Define the image folder\n",
        "img_folder = '/content/images'\n",
        "os.makedirs(img_folder, exist_ok=True)\n",
        "\n",
        "# Add a sample image dictionary\n",
        "images = {\n",
        "    \"image1.png\": \"https://picsum.photos/seed/seed1/400/300\",\n",
        "    \"image2.png\": \"https://picsum.photos/seed/seed2/400/300\",\n",
        "    \"image3.png\": \"https://picsum.photos/seed/seed3/400/300\",\n",
        "    \"image4.png\": \"https://picsum.photos/seed/seed4/400/300\",\n",
        "    \"image5.png\": \"https://picsum.photos/seed/seed5/400/300\",\n",
        "    \"image6.png\": \"https://picsum.photos/seed/seed6/400/300\",\n",
        "    \"image7.png\": \"https://picsum.photos/seed/seed7/400/300\",\n",
        "    \"image8.png\": \"https://picsum.photos/seed/seed8/400/300\",\n",
        "    \"image9.png\": \"https://picsum.photos/seed/seed9/400/300\",\n",
        "    \"image10.png\": \"https://picsum.photos/seed/seed10/400/300\"\n",
        "}\n",
        "\n",
        "\n",
        "img_paths = []\n",
        "doc_embeddings = []\n",
        "\n",
        "for name, url in tqdm.tqdm(images.items()):\n",
        "    img_path = os.path.join(img_folder, name)\n",
        "    img_paths.append(img_path)\n",
        "\n",
        "    # Download the image if needed\n",
        "    if not os.path.exists(img_path):\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(img_path, \"wb\") as fOut:\n",
        "            fOut.write(response.content)\n",
        "\n",
        "    # Get the base64 representation of the image\n",
        "    api_input_document = {\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": base64_from_image(img_path)},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Call the Embed v4.0 model\n",
        "    api_response = co.embed(\n",
        "        model=\"embed-v4.0\",\n",
        "        input_type=\"search_document\",\n",
        "        embedding_types=[\"float\"],\n",
        "        inputs=[api_input_document],\n",
        "    )\n",
        "\n",
        "    # Store embedding\n",
        "    emb = np.asarray(api_response.embeddings.float[0])\n",
        "    doc_embeddings.append(emb)\n",
        "\n",
        "doc_embeddings = np.vstack(doc_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1kgI_pJTH4b"
      },
      "outputs": [],
      "source": [
        "def search(question, max_img_size=800):\n",
        "    # Compute the embedding for the query\n",
        "    api_response = co.embed(\n",
        "        model=\"embed-v4.0\",\n",
        "        input_type=\"search_query\",\n",
        "        embedding_types=[\"float\"],\n",
        "        texts=[question],\n",
        "    )\n",
        "\n",
        "    query_emb = np.asarray(api_response.embeddings.float[0])\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    cos_sim_scores = np.dot(query_emb, doc_embeddings.T)\n",
        "\n",
        "    # Get the most relevant image\n",
        "    top_idx = np.argmax(cos_sim_scores)\n",
        "    hit_img_path = img_paths[top_idx]\n",
        "\n",
        "    return hit_img_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByHYtuQ6V9q0"
      },
      "outputs": [],
      "source": [
        "def answer(question, img_path):\n",
        "    prompt = [f\"\"\"Answer the question based solely on the information from the image.\n",
        "               Question: {question}\"\"\", PIL.Image.open(img_path)]\n",
        "\n",
        "    response = client.generate_content(\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "E-koH4_pTroJ",
        "outputId": "cedfa686-ed4c-4a25-ceaf-4b14f5030311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the information provided in the image, it is impossible to identify which one is seed7. The image only shows rows of crops.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = \"Which one is seed7?\"\n",
        "top_image_path = search(question)\n",
        "answer_text = answer(question, top_image_path)\n",
        "print(answer_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVz41j__ve2a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wf1JTX90wRm"
      },
      "source": [
        "# Receipt & Document Scanner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j5dSEKnk7NIx",
        "outputId": "169ff29c-57cb-4556-ebd5-4c5cdd95d0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting document info: HTTPConnectionPool(host='localhost', port=42687): Read timed out. (read timeout=600.0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'document_type': 'unknown',\n",
              " 'vendor_name': None,\n",
              " 'date': None,\n",
              " 'amount': None,\n",
              " 'items': [],\n",
              " 'bill_type': None,\n",
              " 'account_number': None,\n",
              " 'warranty_period': None,\n",
              " 'product_name': None,\n",
              " 'key_text': '',\n",
              " 'expiry_date': None}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_document_info('/content/Receipt-template-example.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaAkUVqs0xbL"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def extract_document_info(img_path: str) -> Dict:\n",
        "    \"\"\"Extract structured information from document/receipt using Gemini\"\"\"\n",
        "\n",
        "    prompt = \"\"\"Analyze this document/receipt and extract the following information in JSON format:\n",
        "    {\n",
        "        \"document_type\": \"receipt/bill/warranty/invoice/contract/other\",\n",
        "        \"vendor_name\": \"store/company name\",\n",
        "        \"date\": \"YYYY-MM-DD format if found\",\n",
        "        \"amount\": \"total amount if found\",\n",
        "        \"items\": [\"list of items/services if receipt\"],\n",
        "        \"bill_type\": \"electricity/water/gas/internet/phone/other if utility bill\",\n",
        "        \"account_number\": \"account/reference number if found\",\n",
        "        \"warranty_period\": \"warranty duration if warranty document\",\n",
        "        \"product_name\": \"product name if warranty/purchase\",\n",
        "        \"key_text\": \"important text snippets for search\",\n",
        "        \"expiry_date\": \"YYYY-MM-DD if found (warranties, subscriptions, etc)\"\n",
        "    }\n",
        "\n",
        "    If information is not found, use null. Be precise with dates and amounts.\"\"\"\n",
        "\n",
        "    try:\n",
        "        pil_image = PIL.Image.open(img_path)\n",
        "        response = client.generate_content([prompt, pil_image])\n",
        "\n",
        "        # Clean up the response and parse JSON\n",
        "        response_text = response.text.strip()\n",
        "        if response_text.startswith('```json'):\n",
        "            response_text = response_text[7:-3]\n",
        "        elif response_text.startswith('```'):\n",
        "            response_text = response_text[3:-3]\n",
        "\n",
        "        return json.loads(response_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting document info: {e}\")\n",
        "        return {\n",
        "            \"document_type\": \"unknown\",\n",
        "            \"vendor_name\": None,\n",
        "            \"date\": None,\n",
        "            \"amount\": None,\n",
        "            \"items\": [],\n",
        "            \"bill_type\": None,\n",
        "            \"account_number\": None,\n",
        "            \"warranty_period\": None,\n",
        "            \"product_name\": None,\n",
        "            \"key_text\": \"\",\n",
        "            \"expiry_date\": None\n",
        "        }\n",
        "\n",
        "def process_document(img_path: str):\n",
        "    \"\"\"Process a single document - extract info and create embedding\"\"\"\n",
        "\n",
        "    # Extract structured information\n",
        "    doc_info = extract_document_info(img_path)\n",
        "\n",
        "    # Create search text from extracted info\n",
        "    search_text_parts = []\n",
        "    if doc_info.get('vendor_name'):\n",
        "        search_text_parts.append(doc_info['vendor_name'])\n",
        "    if doc_info.get('document_type'):\n",
        "        search_text_parts.append(doc_info['document_type'])\n",
        "    if doc_info.get('bill_type'):\n",
        "        search_text_parts.append(doc_info['bill_type'])\n",
        "    if doc_info.get('product_name'):\n",
        "        search_text_parts.append(doc_info['product_name'])\n",
        "    if doc_info.get('items'):\n",
        "        search_text_parts.extend(doc_info['items'])\n",
        "    if doc_info.get('key_text'):\n",
        "        search_text_parts.append(doc_info['key_text'])\n",
        "\n",
        "    search_text = \" \".join(search_text_parts)\n",
        "\n",
        "    # Create embedding using text + image\n",
        "    api_input_document = {\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": search_text},\n",
        "            {\"type\": \"image\", \"image\": base64_from_image(img_path)},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Get embedding\n",
        "    api_response = co.embed(\n",
        "        model=\"embed-v4.0\",\n",
        "        input_type=\"search_document\",\n",
        "        embedding_types=[\"float\"],\n",
        "        inputs=[api_input_document],\n",
        "    )\n",
        "\n",
        "    emb = np.asarray(api_response.embeddings.float[0])\n",
        "\n",
        "    # This part needs to be adapted to your data storage strategy\n",
        "    # For now, let's assume you have lists to store this data\n",
        "    # self.doc_embeddings.append(emb)\n",
        "    # self.doc_metadata.append(doc_info)\n",
        "    # self.doc_paths.append(img_path)\n",
        "\n",
        "    return doc_info\n",
        "\n",
        "def base64_from_image(img_path: str) -> str:\n",
        "    \"\"\"Convert image to base64 string\"\"\"\n",
        "    pil_image = PIL.Image.open(img_path)\n",
        "    img_format = pil_image.format if pil_image.format else \"PNG\"\n",
        "\n",
        "    # Resize if too large\n",
        "    max_pixels = 1568 * 1568\n",
        "    org_width, org_height = pil_image.size\n",
        "    if org_width * org_height > max_pixels:\n",
        "        scale_factor = (max_pixels / (org_width * org_height)) ** 0.5\n",
        "        new_width = int(org_width * scale_factor)\n",
        "        new_height = int(org_height * scale_factor)\n",
        "        pil_image.thumbnail((new_width, new_height))\n",
        "\n",
        "    with io.BytesIO() as img_buffer:\n",
        "        pil_image.save(img_buffer, format=img_format)\n",
        "        img_buffer.seek(0)\n",
        "        img_data = f\"data:image/{img_format.lower()};base64,\" + base64.b64encode(img_buffer.read()).decode(\"utf-8\")\n",
        "\n",
        "    return img_data\n",
        "\n",
        "def search_documents(query: str, top_k: int = 5) -> List[Tuple[str, Dict, float]]:\n",
        "    \"\"\"Search documents by natural language query\"\"\"\n",
        "\n",
        "    # This part needs to be adapted to your data storage strategy\n",
        "    # if not self.doc_embeddings:\n",
        "    #     return []\n",
        "\n",
        "    # Create query embedding\n",
        "    api_response = co.embed(\n",
        "        model=\"embed-v4.0\",\n",
        "        input_type=\"search_query\",\n",
        "        embedding_types=[\"float\"],\n",
        "        texts=[query],\n",
        "    )\n",
        "\n",
        "    query_emb = np.asarray(api_response.embeddings.float[0])\n",
        "    # doc_embeddings_matrix = np.vstack(self.doc_embeddings)\n",
        "\n",
        "    # Calculate similarities\n",
        "    # cos_sim_scores = np.dot(query_emb, doc_embeddings_matrix.T)\n",
        "\n",
        "    # Get top results\n",
        "    # top_indices = np.argsort(cos_sim_scores)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    # for idx in top_indices:\n",
        "    #     results.append((\n",
        "    #         self.doc_paths[idx],\n",
        "    #         self.doc_metadata[idx],\n",
        "    #         float(cos_sim_scores[idx])\n",
        "    #     ))\n",
        "\n",
        "    return results\n",
        "\n",
        "def search_by_date_range(start_date: str, end_date: str = None) -> List[Tuple[str, Dict]]:\n",
        "    \"\"\"Search documents by date range\"\"\"\n",
        "    if end_date is None:\n",
        "        end_date = start_date\n",
        "\n",
        "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    results = []\n",
        "    # for i, metadata in enumerate(self.doc_metadata):\n",
        "    #     if metadata.get('date'):\n",
        "    #         try:\n",
        "    #             doc_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\")\n",
        "    #             if start_dt <= doc_date <= end_dt:\n",
        "    #                 results.append((self.doc_paths[i], metadata))\n",
        "    #         except:\n",
        "    #             continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def search_by_vendor(vendor_name: str) -> List[Tuple[str, Dict]]:\n",
        "    \"\"\"Search documents by vendor/company name\"\"\"\n",
        "    results = []\n",
        "    vendor_lower = vendor_name.lower()\n",
        "\n",
        "    # for i, metadata in enumerate(self.doc_metadata):\n",
        "    #     if metadata.get('vendor_name'):\n",
        "    #         if vendor_lower in metadata['vendor_name'].lower():\n",
        "    #             results.append((self.doc_paths[i], metadata))\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_expiring_warranties(days_ahead: int = 30) -> List[Tuple[str, Dict]]:\n",
        "    \"\"\"Find warranties expiring within specified days\"\"\"\n",
        "    cutoff_date = datetime.now() + timedelta(days=days_ahead)\n",
        "    results = []\n",
        "\n",
        "    # for i, metadata in enumerate(self.doc_metadata):\n",
        "    #     if metadata.get('expiry_date'):\n",
        "    #         try:\n",
        "    #             expiry_dt = datetime.strptime(metadata['expiry_date'], \"%Y-%m-%d\")\n",
        "    #             if expiry_dt <= cutoff_date:\n",
        "    #                 results.append((self.doc_paths[i], metadata))\n",
        "    #         except:\n",
        "    #             continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_monthly_bills(year: int, month: int) -> Dict[str, List[Tuple[str, Dict]]]:\n",
        "    \"\"\"Get all bills for a specific month organized by type\"\"\"\n",
        "    target_date = f\"{year}-{month:02d}\"\n",
        "    bills_by_type = {}\n",
        "\n",
        "    # for i, metadata in enumerate(self.doc_metadata):\n",
        "    #     if (metadata.get('document_type') == 'bill' and\n",
        "    #         metadata.get('date') and\n",
        "    #         metadata['date'].startswith(target_date)):\n",
        "\n",
        "    #         bill_type = metadata.get('bill_type', 'other')\n",
        "    #         if bill_type not in bills_by_type:\n",
        "    #             bills_by_type[bill_type] = []\n",
        "\n",
        "    #         bills_by_type[bill_type].append((self.doc_paths[i], metadata))\n",
        "\n",
        "    return bills_by_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwvELoTd07TH",
        "outputId": "ab6715a0-75d6-4995-f2da-0dede7cb3985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¾ Receipt & Document Scanner Demo\n",
            "==================================================\n",
            "\n",
            "ðŸ“‹ Scenario: Find that warranty photo\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_documents('Find that warranty photo')\n",
            "\n",
            "ðŸ“‹ Scenario: Show me my electricity bill from March\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_documents('electricity bill March')\n",
            "\n",
            "ðŸ“‹ Scenario: All receipts from Target\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_by_vendor('Target')\n",
            "\n",
            "ðŸ“‹ Scenario: What warranties expire this month?\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_documents('What warranties expire this month?')\n",
            "\n",
            "ðŸ“‹ Scenario: Find my phone bill from last month\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_documents('electricity bill March')\n",
            "\n",
            "ðŸ“‹ Scenario: Show me all grocery receipts\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_documents('Show me all grocery receipts')\n",
            "\n",
            "ðŸ“‹ Scenario: Find receipts over $100\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.search_documents('Find receipts over $100')\n",
            "\n",
            "ðŸ“‹ Scenario: What bills do I have for January 2024?\n",
            "ðŸ’¡ Implementation:\n",
            "   scanner.get_monthly_bills(2024, 1)\n"
          ]
        }
      ],
      "source": [
        "# Initialize (assuming you have your API keys set up)\n",
        "# cohere_api_key = \"your_key\"\n",
        "# gemini_api_key = \"your_key\"\n",
        "# co = cohere.ClientV2(api_key=cohere_api_key)\n",
        "# genai.configure(api_key=gemini_api_key)\n",
        "# gemini_client = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# scanner = DocumentScanner(co, gemini_client)\n",
        "\n",
        "print(\"ðŸ§¾ Receipt & Document Scanner Demo\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Example usage scenarios:\n",
        "scenarios = [\n",
        "    \"Find that warranty photo\",\n",
        "    \"Show me my electricity bill from March\",\n",
        "    \"All receipts from Target\",\n",
        "    \"What warranties expire this month?\",\n",
        "    \"Find my phone bill from last month\",\n",
        "    \"Show me all grocery receipts\",\n",
        "    \"Find receipts over $100\",\n",
        "    \"What bills do I have for January 2024?\"\n",
        "]\n",
        "\n",
        "for scenario in scenarios:\n",
        "    print(f\"\\nðŸ“‹ Scenario: {scenario}\")\n",
        "    print(\"ðŸ’¡ Implementation:\")\n",
        "\n",
        "    if \"warranty\" in scenario.lower() and \"expire\" in scenario.lower():\n",
        "        print(\"   scanner.get_expiring_warranties(30)\")\n",
        "    elif \"electricity bill\" in scenario.lower() or \"phone bill\" in scenario.lower():\n",
        "        print(\"   scanner.search_documents('electricity bill March')\")\n",
        "    elif \"from\" in scenario.lower() and any(store in scenario.lower() for store in ['target', 'walmart']):\n",
        "        vendor = scenario.split('from ')[-1].strip()\n",
        "        print(f\"   scanner.search_by_vendor('{vendor}')\")\n",
        "    elif \"bills\" in scenario.lower() and \"january\" in scenario.lower():\n",
        "        print(\"   scanner.get_monthly_bills(2024, 1)\")\n",
        "    else:\n",
        "        print(f\"   scanner.search_documents('{scenario}')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gVlGFQF1b9l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a5fb2b5"
      },
      "outputs": [],
      "source": [
        "class DocumentScanner:\n",
        "    def __init__(self, cohere_client, gemini_client):\n",
        "        self.co = cohere_client\n",
        "        self.client = gemini_client\n",
        "        self.doc_embeddings = []\n",
        "        self.doc_metadata = []\n",
        "        self.doc_paths = []\n",
        "\n",
        "    def process_document(self, img_path: str):\n",
        "        \"\"\"Process a single document - extract info and create embedding\"\"\"\n",
        "\n",
        "        # Extract structured information\n",
        "        doc_info = self.extract_document_info(img_path)\n",
        "\n",
        "        # Create search text from extracted info\n",
        "        search_text_parts = []\n",
        "        if doc_info.get('vendor_name'):\n",
        "            search_text_parts.append(doc_info['vendor_name'])\n",
        "        if doc_info.get('document_type'):\n",
        "            search_text_parts.append(doc_info['document_type'])\n",
        "        if doc_info.get('bill_type'):\n",
        "            search_text_parts.append(doc_info['bill_type'])\n",
        "        if doc_info.get('product_name'):\n",
        "            search_text_parts.append(doc_info['product_name'])\n",
        "        if doc_info.get('items'):\n",
        "            search_text_parts.extend(doc_info['items'])\n",
        "        if doc_info.get('key_text'):\n",
        "            search_text_parts.append(doc_info['key_text'])\n",
        "\n",
        "        search_text = \" \".join(filter(None, search_text_parts))\n",
        "\n",
        "        # Create embedding using text + image\n",
        "        api_input_document = {\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": search_text},\n",
        "                {\"type\": \"image\", \"image\": self.base64_from_image(img_path)},\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Get embedding\n",
        "        api_response = self.co.embed(\n",
        "            model=\"embed-v4.0\",\n",
        "            input_type=\"search_document\",\n",
        "            embedding_types=[\"float\"],\n",
        "            inputs=[api_input_document],\n",
        "        )\n",
        "\n",
        "        emb = np.asarray(api_response.embeddings.float[0])\n",
        "\n",
        "        self.doc_embeddings.append(emb)\n",
        "        self.doc_metadata.append(doc_info)\n",
        "        self.doc_paths.append(img_path)\n",
        "\n",
        "        return doc_info\n",
        "\n",
        "    def extract_document_info(self, img_path: str) -> Dict:\n",
        "        \"\"\"Extract structured information from document/receipt using Gemini\"\"\"\n",
        "\n",
        "        prompt = \"\"\"Analyze this document/receipt and extract the following information in JSON format:\n",
        "        {\n",
        "            \"document_type\": \"receipt/bill/warranty/invoice/contract/other\",\n",
        "            \"vendor_name\": \"store/company name\",\n",
        "            \"date\": \"YYYY-MM-DD format if found\",\n",
        "            \"amount\": \"total amount if found\",\n",
        "            \"items\": [\"list of items/services if receipt\"],\n",
        "            \"bill_type\": \"electricity/water/gas/internet/phone/other if utility bill\",\n",
        "            \"account_number\": \"account/reference number if found\",\n",
        "            \"warranty_period\": \"warranty duration if warranty document\",\n",
        "            \"product_name\": \"product name if warranty/purchase\",\n",
        "            \"key_text\": \"important text snippets for search\",\n",
        "            \"expiry_date\": \"YYYY-MM-DD if found (warranties, subscriptions, etc)\"\n",
        "        }\n",
        "\n",
        "        If information is not found, use null. Be precise with dates and amounts.\"\"\"\n",
        "\n",
        "        try:\n",
        "            pil_image = PIL.Image.open(img_path)\n",
        "            response = self.client.generate_content([prompt, pil_image])\n",
        "\n",
        "            # Clean up the response and parse JSON\n",
        "            response_text = response.text.strip()\n",
        "            if response_text.startswith('```json'):\n",
        "                response_text = response_text[7:-3]\n",
        "            elif response_text.startswith('```'):\n",
        "                response_text = response_text[3:-3]\n",
        "\n",
        "            return json.loads(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting document info: {e}\")\n",
        "            return {\n",
        "                \"document_type\": \"unknown\",\n",
        "                \"vendor_name\": None,\n",
        "                \"date\": None,\n",
        "                \"amount\": None,\n",
        "                \"items\": [],\n",
        "                \"bill_type\": None,\n",
        "                \"account_number\": None,\n",
        "                \"warranty_period\": None,\n",
        "                \"product_name\": None,\n",
        "                \"key_text\": \"\",\n",
        "                \"expiry_date\": None\n",
        "            }\n",
        "\n",
        "    def base64_from_image(self, img_path: str) -> str:\n",
        "        \"\"\"Convert image to base64 string\"\"\"\n",
        "        pil_image = PIL.Image.open(img_path)\n",
        "        img_format = pil_image.format if pil_image.format else \"PNG\"\n",
        "\n",
        "        # Resize if too large\n",
        "        max_pixels = 1568 * 1568\n",
        "        org_width, org_height = pil_image.size\n",
        "        if org_width * org_height > max_pixels:\n",
        "            scale_factor = (max_pixels / (org_width * org_height)) ** 0.5\n",
        "            new_width = int(org_width * scale_factor)\n",
        "            new_height = int(org_height * scale_factor)\n",
        "            pil_image.thumbnail((new_width, new_height))\n",
        "\n",
        "        with io.BytesIO() as img_buffer:\n",
        "            pil_image.save(img_buffer, format=img_format)\n",
        "            img_buffer.seek(0)\n",
        "            img_data = f\"data:image/{img_format.lower()};base64,\" + base64.b64encode(img_buffer.read()).decode(\"utf-8\")\n",
        "\n",
        "        return img_data\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, Dict, float]]:\n",
        "        \"\"\"Search documents by natural language query\"\"\"\n",
        "\n",
        "        if not self.doc_embeddings:\n",
        "            return []\n",
        "\n",
        "        # Create query embedding\n",
        "        api_response = self.co.embed(\n",
        "            model=\"embed-v4.0\",\n",
        "            input_type=\"search_query\",\n",
        "            embedding_types=[\"float\"],\n",
        "            texts=[query],\n",
        "        )\n",
        "\n",
        "        query_emb = np.asarray(api_response.embeddings.float[0])\n",
        "        doc_embeddings_matrix = np.vstack(self.doc_embeddings)\n",
        "\n",
        "        # Calculate similarities\n",
        "        cos_sim_scores = np.dot(query_emb, doc_embeddings_matrix.T)\n",
        "\n",
        "        # Get top results\n",
        "        top_indices = np.argsort(cos_sim_scores)[::-1][:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append((\n",
        "                self.doc_paths[idx],\n",
        "                self.doc_metadata[idx],\n",
        "                float(cos_sim_scores[idx])\n",
        "            ))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_by_date_range(self, start_date: str, end_date: str = None) -> List[Tuple[str, Dict]]:\n",
        "        \"\"\"Search documents by date range\"\"\"\n",
        "        if end_date is None:\n",
        "            end_date = start_date\n",
        "\n",
        "        start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "        end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "        results = []\n",
        "        for i, metadata in enumerate(self.doc_metadata):\n",
        "            if metadata.get('date'):\n",
        "                try:\n",
        "                    doc_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\")\n",
        "                    if start_dt <= doc_date <= end_dt:\n",
        "                        results.append((self.doc_paths[i], metadata))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_by_vendor(self, vendor_name: str) -> List[Tuple[str, Dict]]:\n",
        "        \"\"\"Search documents by vendor/company name\"\"\"\n",
        "        results = []\n",
        "        vendor_lower = vendor_name.lower()\n",
        "\n",
        "        for i, metadata in enumerate(self.doc_metadata):\n",
        "            if metadata.get('vendor_name'):\n",
        "                if vendor_lower in metadata['vendor_name'].lower():\n",
        "                    results.append((self.doc_paths[i], metadata))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_expiring_warranties(self, days_ahead: int = 30) -> List[Tuple[str, Dict]]:\n",
        "        \"\"\"Find warranties expiring within specified days\"\"\"\n",
        "        cutoff_date = datetime.now() + timedelta(days=days_ahead)\n",
        "        results = []\n",
        "\n",
        "        for i, metadata in enumerate(self.doc_metadata):\n",
        "            if metadata.get('expiry_date'):\n",
        "                try:\n",
        "                    expiry_dt = datetime.strptime(metadata['expiry_date'], \"%Y-%m-%d\")\n",
        "                    if expiry_dt <= cutoff_date:\n",
        "                        results.append((self.doc_paths[i], metadata))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_monthly_bills(self, year: int, month: int) -> Dict[str, List[Tuple[str, Dict]]]:\n",
        "        \"\"\"Get all bills for a specific month organized by type\"\"\"\n",
        "        target_date = f\"{year}-{month:02d}\"\n",
        "        bills_by_type = {}\n",
        "\n",
        "        for i, metadata in enumerate(self.doc_metadata):\n",
        "            if (metadata.get('document_type') == 'bill' and\n",
        "                metadata.get('date') and\n",
        "                metadata['date'].startswith(target_date)):\n",
        "\n",
        "                bill_type = metadata.get('bill_type', 'other')\n",
        "                if bill_type not in bills_by_type:\n",
        "                    bills_by_type[bill_type] = []\n",
        "\n",
        "                bills_by_type[bill_type].append((self.doc_paths[i], metadata))\n",
        "\n",
        "        return bills_by_type\n",
        "\n",
        "# Instantiate the scanner\n",
        "scanner = DocumentScanner(co, client)\n",
        "\n",
        "# Now you can use the scanner object to process your documents, for example:\n",
        "# scanner.process_document('path/to/your/receipt.jpg')\n",
        "# search_results = scanner.search_documents(\"groceries from last week\")\n",
        "# print(search_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gXfaxKK1hrD"
      },
      "outputs": [],
      "source": [
        "def process_and_display_no_gradio(image_path):\n",
        "    \"\"\"Processes a document and prints the extracted information.\"\"\"\n",
        "    if image_path is None:\n",
        "        print(\"Please provide an image path.\")\n",
        "        return\n",
        "    doc_info = scanner.process_document(image_path)\n",
        "    print(f\"Processed document: {image_path}\")\n",
        "    print(json.dumps(doc_info, indent=2))\n",
        "\n",
        "def search_and_display_no_gradio(query):\n",
        "    \"\"\"Searches for documents and prints the results.\"\"\"\n",
        "    search_results = scanner.search_documents(query)\n",
        "    if not search_results:\n",
        "        print(\"No results found.\")\n",
        "        return\n",
        "\n",
        "    for path, metadata, score in search_results:\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        print(f\"Metadata: {json.dumps(metadata, indent=2)}\")\n",
        "        print(f\"Image Path: {path}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# 1. Process a document\n",
        "#    (replace with the actual path to your image)\n",
        "# process_and_display_no_gradio('/content/images/image1.png')\n",
        "\n",
        "# 2. Search for documents\n",
        "# search_and_display_no_gradio(\"your search query\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "79f995ea",
        "outputId": "5ab973f4-76f5-4300-9a79-274a93cac0c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f9ab4846fee6335b3d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f9ab4846fee6335b3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://f9ab4846fee6335b3d.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def process_and_display(image_path):\n",
        "    if image_path is None:\n",
        "        return \"Please upload an image.\", \"{}\"\n",
        "    doc_info = scanner.process_document(image_path)\n",
        "    return f\"Processed document: {image_path}\", json.dumps(doc_info, indent=2)\n",
        "\n",
        "def search_and_display(query):\n",
        "    search_results = scanner.search_documents(query)\n",
        "    if not search_results:\n",
        "        return \"No results found.\", []\n",
        "\n",
        "    results_html = \"\"\n",
        "    images = []\n",
        "    for path, metadata, score in search_results:\n",
        "        results_html += f\"<b>Score:</b> {score:.4f}<br>\"\n",
        "        results_html += f\"<b>Metadata:</b><pre>{json.dumps(metadata, indent=2)}</pre><hr>\"\n",
        "        images.append(path)\n",
        "\n",
        "    return results_html, images\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Receipt & Document Scanner\")\n",
        "\n",
        "    with gr.Tab(\"Upload & Process\"):\n",
        "        with gr.Row():\n",
        "            image_input = gr.Image(type=\"filepath\", label=\"Upload Document\")\n",
        "            json_output = gr.JSON(label=\"Extracted Information\")\n",
        "        process_button = gr.Button(\"Process Document\")\n",
        "        status_output = gr.Textbox(label=\"Status\")\n",
        "        process_button.click(process_and_display, inputs=image_input, outputs=[status_output, json_output])\n",
        "\n",
        "\n",
        "    with gr.Tab(\"Search\"):\n",
        "        search_input = gr.Textbox(label=\"Search Query\")\n",
        "        search_button = gr.Button(\"Search\")\n",
        "        search_results_html = gr.HTML()\n",
        "        search_results_images = gr.Gallery(label=\"Search Results\")\n",
        "        search_button.click(search_and_display, inputs=search_input, outputs=[search_results_html, search_results_images])\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9wfWCd9459G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "127988e0",
        "outputId": "7abfabc0-f52a-4689-f01e-030f7ce6cb72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting document info: HTTPConnectionPool(host='localhost', port=42687): Read timed out. (read timeout=600.0)\n",
            "Error extracting document info: HTTPConnectionPool(host='localhost', port=42687): Read timed out. (read timeout=600.0)\n",
            "Error extracting document info: HTTPConnectionPool(host='localhost', port=42687): Read timed out. (read timeout=600.0)\n",
            "Processed document: /content/images/image1.png\n",
            "{\n",
            "  \"document_type\": \"unknown\",\n",
            "  \"vendor_name\": null,\n",
            "  \"date\": null,\n",
            "  \"amount\": null,\n",
            "  \"items\": [],\n",
            "  \"bill_type\": null,\n",
            "  \"account_number\": null,\n",
            "  \"warranty_period\": null,\n",
            "  \"product_name\": null,\n",
            "  \"key_text\": \"\",\n",
            "  \"expiry_date\": null\n",
            "}\n",
            "Score: 0.0776\n",
            "Metadata: {\n",
            "  \"document_type\": \"unknown\",\n",
            "  \"vendor_name\": null,\n",
            "  \"date\": null,\n",
            "  \"amount\": null,\n",
            "  \"items\": [],\n",
            "  \"bill_type\": null,\n",
            "  \"account_number\": null,\n",
            "  \"warranty_period\": null,\n",
            "  \"product_name\": null,\n",
            "  \"key_text\": \"\",\n",
            "  \"expiry_date\": null\n",
            "}\n",
            "Image Path: /tmp/gradio/10970daf0c7f5ff4f30a7a92275006db915f1be89c3bb1f29e3ce51c80611132/Receipt-template-example.jpg\n",
            "--------------------\n",
            "Score: 0.0734\n",
            "Metadata: {\n",
            "  \"document_type\": \"unknown\",\n",
            "  \"vendor_name\": null,\n",
            "  \"date\": null,\n",
            "  \"amount\": null,\n",
            "  \"items\": [],\n",
            "  \"bill_type\": null,\n",
            "  \"account_number\": null,\n",
            "  \"warranty_period\": null,\n",
            "  \"product_name\": null,\n",
            "  \"key_text\": \"\",\n",
            "  \"expiry_date\": null\n",
            "}\n",
            "Image Path: /content/images/image1.png\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Process the first sample image\n",
        "process_and_display_no_gradio('/content/images/image1.png')\n",
        "\n",
        "# Now, let's try searching for it\n",
        "search_and_display_no_gradio(\"seed1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7bSCFPz51OD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}